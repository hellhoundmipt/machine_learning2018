{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import jsonlines\n",
    "import itertools\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2 as pm\n",
    "import networkx as nx\n",
    "import re\n",
    "from bisect import bisect_left\n",
    "#from joblib import Parallel, delayed\n",
    "from collections import Counter\n",
    "from tqdm import trange\n",
    "from tqdm import tqdm\n",
    "from math import log\n",
    "from random import shuffle\n",
    "from sys import exit\n",
    "import numpy as np\n",
    "import json\n",
    "import codecs\n",
    "import random\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "\n",
    "from scipy.sparse import load_npz\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import vstack\n",
    "#nltk.download()\n",
    "tknzr = nltk.TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AA', 'AB', 'AD', 'AE', 'AF', 'AI', 'AJ', 'AK', 'AQ']\n"
     ]
    }
   ],
   "source": [
    "path = listdir('C:/Users/Maxim/MyPy/wta/c')\n",
    "path = sorted(path)[:9]\n",
    "print(path)\n",
    "\n",
    "data = []\n",
    "for p in path:\n",
    "    with jsonlines.open('C:/Users/Maxim/MyPy/wta/c/' + p, 'r') as f:\n",
    "        for entry in f:\n",
    "            data.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok_set = set()\n",
    "with open(\"sources/accepted_categories.txt\", mode=\"r\", encoding=\"utf-8\") as inp:\n",
    "    for line in inp:\n",
    "        line = line[:-1]\n",
    "        ok_set.add(line)\n",
    "\n",
    "ok = sorted(ok_set)\n",
    "\n",
    "categories_dict = {}\n",
    "with open(\"sources/article_cat.json\", mode=\"r\") as input:\n",
    "    categories_dict = json.loads(input.read())\n",
    "    \n",
    "category_article = {}\n",
    "with open(\"sources/cat_article.json\", mode=\"r\") as input:\n",
    "    category_article = json.loads(input.read())\n",
    "    \n",
    "texts = {item['id']: item['text'] for item in jsonlines.open('sources/normalized_texts.jl', 'r')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = sorted([id for id in texts.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготваливаем заголовки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 116584/116584 [02:54<00:00, 669.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "sw_ru = nltk.corpus.stopwords.words('russian')\n",
    "morph = pm.MorphAnalyzer()\n",
    "id_title = {}\n",
    "\n",
    "stems = []\n",
    "unsup_symb = set('.,\\(\\):«»?!')\n",
    "for i in trange(len(data)):\n",
    "    id = data[i]['id']\n",
    "    if id in ids:\n",
    "        tokens = tknzr.tokenize(text=data[i]['title'])\n",
    "        tokens = [t.lower() for t in tokens if t not in unsup_symb]\n",
    "        tokens = [morph.parse(t)[0].normal_form for t in tokens]\n",
    "        tokens = [t for t in tokens if t not in sw_ru]\n",
    "        if len(tokens) > 0:\n",
    "            id_title[id] = ' '.join(tokens)\n",
    "            stems.extend(tokens)\n",
    "    \n",
    "stems = set(stems)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_id = {}\n",
    "for id in id_title:\n",
    "    if title_id.get(id_title[id]) is not None:\n",
    "        title_id[id_title[id]].append(id)\n",
    "    else:\n",
    "        title_id[id_title[id]] = [id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вычисляем словарь категорий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 96794/96794 [00:27<00:00, 3554.66it/s]\n"
     ]
    }
   ],
   "source": [
    "categories_vocab = {}\n",
    "\n",
    "for id in tqdm(categories_dict):\n",
    "    for c in categories_dict[id]:\n",
    "        if c in ok:\n",
    "            if id_title.get(id) is not None:\n",
    "                if categories_vocab.get(c) is None:\n",
    "                    categories_vocab[c] = id_title[id]\n",
    "                else:\n",
    "                    categories_vocab[c] += ' ' + id_title[id]\n",
    "\n",
    "\n",
    "for c in categories_vocab:\n",
    "    categories_vocab[c] = set(categories_vocab[c].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''with open(\"sources/categories_vocab\", \"w\") as output:\n",
    "    output.write(json.dumps({c: sorted(vocab) for c, vocab in categories_vocab.items()}))\n",
    "'''\n",
    "with open(\"sources/categories_vocab\", \"r\") as input:\n",
    "    categories_vocab = json.loads(input.read())\n",
    "    categories_vocab = {c: set(vocab) for c, vocab in categories_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [id for id in ids if id in id_title.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 96769/96769 [00:11<00:00, 8232.73it/s]\n"
     ]
    }
   ],
   "source": [
    "for id in tqdm(ids):\n",
    "    tokens = texts[id].split()\n",
    "    tokens = [tok for tok in tokens if tok in stems]\n",
    "    texts[id] = ' '.join(tokens)\n",
    "    \n",
    "data[:] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Начинаем анализировать документы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [    0     1     2 ... 96766 96767 96768] TEST: [    4     7    27 ... 96754 96755 96764]\n",
      "77415 19354\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=27)\n",
    "\n",
    "train_index, test_index = 0, 0\n",
    "for item in kf.split(ids):\n",
    "    train_index, test_index = item[0], item[1]\n",
    "    break\n",
    "\n",
    "print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "print(len(train_index), len(test_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Взвешиваем слова в документe по формуле $ R_w = ft_w \\log{\\frac{N}{cf_w}} $.\n",
    "2. Взвешиваем заголовки по формуле $R_t = \\sum_{w \\rightarrow t} R_w \\frac{1}{t_w} \\frac{1}{a_t} \\frac{S_t}{L_t}$.\n",
    "3. Взвешиваем статьи $R_a = \\max_{t \\rightarrow a} R_t$ (почти так).\n",
    "4. Взвештваем категории $R_c = \\frac{v_c}{d_c} \\sum_{a \\rightarrow c}R_a$ и получам ответ.\n",
    "5. Модификация: обновляем веса категорий по правилу $R_c' = R_c \\frac{\\sum_{w \\in B_c}d_w}{|B_c|}$, $d'_w = \\frac{d_w}{2}$ для $d_w \\in B_c$ в порядке убывания весов категорий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 3225/3225 [1:26:15<00:00,  1.60s/it]\n"
     ]
    }
   ],
   "source": [
    "topn = 20\n",
    "\n",
    "\n",
    "results = {}\n",
    "for i in tqdm(test_index[:int(len(test_index)/6)]):\n",
    "    document = texts[ids[i]]\n",
    "    #Шаг 1\n",
    "    words = re.split(r' ', document)\n",
    "    words_set = set(words)\n",
    "    d_w = {w: 1 for w in words_set}\n",
    "    # Вычисляем tf_w\n",
    "    R_w = dict(Counter(words))\n",
    "    N = len(categories_dict)\n",
    "    # Вычисляем log(...)\n",
    "    for w in words_set:\n",
    "        cf_w = 0\n",
    "        for c in categories_vocab:\n",
    "            if w in categories_vocab[c]:\n",
    "                cf_w += 1\n",
    "        if cf_w > 0:\n",
    "            R_w[w] = R_w[w] * log(N / cf_w)\n",
    "        else:\n",
    "            R_w[w] = 0\n",
    "    \n",
    "    # Шаг 2\n",
    "    R_t = dict()\n",
    "    supp_w_t = {}\n",
    "    title_vocab = dict(Counter(' '.join(title_id.keys()).split()))\n",
    "    for title in title_id.keys():\n",
    "        title_words = title.split()\n",
    "        supp_words = []\n",
    "        flag = 1\n",
    "        for w in title_words:\n",
    "            if w not in words_set:\n",
    "                flag = flag - 1\n",
    "            else:\n",
    "                supp_words.append(w)\n",
    "        if flag < 0:\n",
    "            continue\n",
    "        sub_sum = 0\n",
    "        # Вынесем 1 / a_t * S_t / L_t как mutual_mul\n",
    "        mutual_mul = len(supp_words) / (len(title_id[title]) * len(title_words))\n",
    "        for w in supp_words:\n",
    "            sub_sum += R_w[w] / title_vocab[w]\n",
    "        \n",
    "        R_t[title] = mutual_mul * sub_sum\n",
    "        supp_w_t[title] = supp_words\n",
    "    \n",
    "    # Шаг 3\n",
    "    #R_a = {d['id']: R_t.get(d['title']) for d in data if R_t.get(d['title']) is not None and R_t.get(d['title']) > 0.0}\n",
    "    #supp_w_a = {d['id']: supp_w_t.get(d['title']) for d in data if supp_w_t.get(d['title']) is not None \n",
    "    #            and  R_t.get(d['title']) > 0.0}\n",
    "    R_a = {id: R_t.get(title) for (id, title) in id_title.items() if R_t.get(title) is not None and R_t.get(title) > 0.0}\n",
    "    supp_w_a = {id: supp_w_t.get(title) for (id, title) in id_title.items() if supp_w_t.get(title) is not None \n",
    "                and R_t.get(title) > 0.0}\n",
    "    \n",
    "    # Шаг 4\n",
    "    R_c = {}\n",
    "    for c in ok:\n",
    "        v_c = 0\n",
    "        d_c = len(categories_vocab[c])\n",
    "        r_c = 0\n",
    "        for id in category_article[c]:\n",
    "            if R_a.get(id) is not None:\n",
    "                r_c += R_a.get(id)\n",
    "                v_c += len(supp_w_a.get(id))\n",
    "        R_c[c] = r_c * v_c / d_c\n",
    "    \n",
    "    # Шаг 5    \n",
    "    R__c = sorted([k for k in R_c.items() if k[1] > 0], key=lambda t: t[1], reverse=True)[:topn]\n",
    "    R_c = {}\n",
    "    for r_c in R__c:\n",
    "        B_c = set(' '.join([' '.join(supp_w_a.get(id)) for id in category_article[r_c[0]] \n",
    "                            if supp_w_a.get(id) is not None]).split())\n",
    "        sub_sum = 0\n",
    "        for b in B_c:\n",
    "            sub_sum += d_w[b]\n",
    "            d_w[b] = d_w[b] / 2\n",
    "        \n",
    "        R_c[r_c[0]] = r_c[1] * sub_sum / len(B_c)\n",
    "    \n",
    "    # Влом исправлять\n",
    "    results[ids[i]] = sorted([k for k in R_c.items() if k[1] > 0], key=lambda t: t[1], reverse=True)[:topn]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Применяем бинарную логистическую регрессию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = joblib.load(\"sources/clf_binlogreg.pkl\")\n",
    "X = load_npz(\"sources/text_tfidf.npz\")\n",
    "class_centroids = load_npz(\"sources/sparce_centroids_nosvd.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96794, 121303)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ids = [id for id in texts.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2150/2150 [01:20<00:00, 26.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.635502592681878\n",
      "0.1163376519681132\n",
      "0.1966717796190236\n"
     ]
    }
   ],
   "source": [
    "precision = 0\n",
    "recall = 0\n",
    "f1 = 0\n",
    "_confirmed = []\n",
    "_true = []\n",
    "\n",
    "N = len(results)\n",
    "for result in tqdm(results):\n",
    "    n = _ids.index(result)\n",
    "    suggested = [res[0] for res in results[result]]\n",
    "    rows = [ok.index(s) for s in suggested]\n",
    "    X_right = class_centroids[rows]\n",
    "    X_left = X[[n]*len(rows)]\n",
    "    X_full = hstack([X_left, X_right])\n",
    "    prediction = clf.predict(X_full)\n",
    "    confirmed = [sugg for i, sugg in enumerate(suggested) if prediction[i] == 1]\n",
    "    true = categories_dict[result]\n",
    "    \n",
    "    if len(confirmed) > 0:\n",
    "        _confirmed.append([1 if cat in confirmed else 0 for cat in ok])\n",
    "    else:\n",
    "        _confirmed.append([1 if cat in suggested else 0 for cat in ok])\n",
    "        \n",
    "    _true.append([1 if cat in true else 0 for cat in ok])\n",
    "    \n",
    "    if len(confirmed) > 0:\n",
    "        precision += metric(true, confirmed) / N\n",
    "        recall += metric(confirmed, true) / N\n",
    "    \n",
    "#precision = precision_score(_true, _confirmed, average='micro')\n",
    "#recall = recall_score(_true, _confirmed, average='micro')\n",
    "#f1 = f1_score(_true, _confirmed, average='micro')\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(res, true):\n",
    "    hits = 0\n",
    "    for c in res:\n",
    "        if c in true:\n",
    "            hits += 1\n",
    "    return hits / len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MGIA:\n",
    "    \n",
    "    def __init__(self, G):\n",
    "        self.G = G.to_undirected()\n",
    "        self.flow = nx.DiGraph()\n",
    "        \n",
    "    def score(self, TRUE, PRED):\n",
    "        if (len(TRUE) == 0) or (len(PRED) == 0):\n",
    "            return 0\n",
    "            \n",
    "        self.flow.clear()\n",
    "        M = len(PRED)\n",
    "        N = len(TRUE)\n",
    "        infinity = 130 #10 #G.number_of_edges()\n",
    "        self.flow.add_node('source', demand=0)\n",
    "        self.flow.add_node('sink', demand=M-N)\n",
    "        self.flow.add_node('DP', demand=0)\n",
    "        self.flow.add_node('DT', demand=0)\n",
    "        self.flow.add_edge('source', 'DP', capacity=N, weight=0)\n",
    "        self.flow.add_edge('DT', 'sink', capacity=M, weight=0)\n",
    "        self.flow.add_edge('sink', 'source', capacity=(M)*(N+1), weight=0)\n",
    "        for pred in PRED:\n",
    "            _pred = 'p' + str(pred)\n",
    "            self.flow.add_node(_pred, demand=-1)\n",
    "            self.flow.add_edge('source', _pred, capacity=N, weight=0)\n",
    "            self.flow.add_edge(_pred, 'DT', capacity=1, weight=infinity)\n",
    "        for true in TRUE:\n",
    "            _true = 't' + str(true)\n",
    "            self.flow.add_node(_true, demand=1)\n",
    "            self.flow.add_edge(_true, 'sink', capacity=M, weight=0)\n",
    "            self.flow.add_edge('DP', _true, capacity=1, weight=infinity)\n",
    "            for pred in PRED:\n",
    "                k_ij = nx.shortest_path_length(self.G, source=pred, target=true)\n",
    "                self.flow.add_edge('p' + str(pred), _true, capacity=1, weight=k_ij * k_ij * k_ij)\n",
    "\n",
    "        flowCost, flowDict = nx.network_simplex(self.flow)\n",
    "        low = (len(PRED.union(TRUE).difference(PRED.intersection(TRUE))))\n",
    "        #print(\"Len = \", len(TRUE), len(PRED), low)\n",
    "        if low == 0:\n",
    "            accuracy = 1\n",
    "        else:\n",
    "            accuracy = 1 - flowCost / (low * infinity)\n",
    "        #print(accuracy)\n",
    "        return accuracy\n",
    "    \n",
    "cat_graph = nx.read_gpickle(\"sources/graph.gpickle\")\n",
    "mgia = MGIA(cat_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_id = {}\n",
    "with open(\"sources/cat_id.json\", mode=\"r\") as input:\n",
    "    cat_id = json.loads(input.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3225/3225 [00:23<00:00, 135.09it/s]\n"
     ]
    }
   ],
   "source": [
    "prec, rec, f1 = 0, 0, 0\n",
    "mgia_score, mean = 0, 0\n",
    "p = 0.35\n",
    "for r in tqdm(results):\n",
    "    res = results[r][:10]\n",
    "    decision_crit = p * max([_r[1] for _r in res])\n",
    "    y_pred = []\n",
    "    for _r in res:\n",
    "        if _r[1] >= decision_crit:\n",
    "            y_pred.append(_r[0])\n",
    "    \n",
    "    y_true = categories_dict[r]\n",
    "    \n",
    "    Y_true = set(cat_id[c] for c in y_true)\n",
    "    Y_pred = set(cat_id[c] for c in y_pred)\n",
    "    \n",
    "    prec += metric(y_pred, y_true)\n",
    "    rec += metric(y_true, y_pred)\n",
    "    mgia_score += mgia.score(Y_true, Y_pred)\n",
    "    mean += len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46406829088224366\n",
      "0.4278531468820072\n",
      "0.4452254878407877\n",
      "0.49337154340790573\n",
      "2.7748837209302324\n"
     ]
    }
   ],
   "source": [
    "prec = prec / len(results)\n",
    "rec = rec / len(results)\n",
    "mgia_score = mgia_score / len(results)\n",
    "f1 = 2 * prec * rec / (prec + rec)\n",
    "print(prec)\n",
    "print(rec)\n",
    "print(f1)\n",
    "print(mgia_score)\n",
    "print(mean / len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
