{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import jsonlines\n",
    "import itertools\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2 as pm\n",
    "import re\n",
    "from bisect import bisect_left\n",
    "#from joblib import Parallel, delayed\n",
    "from collections import Counter\n",
    "from tqdm import trange\n",
    "from tqdm import tqdm\n",
    "from math import log\n",
    "from random import shuffle\n",
    "from sys import exit\n",
    "import numpy as np\n",
    "import json\n",
    "import networkx as nx\n",
    "import nltk\n",
    "\n",
    "\n",
    "from mediawiki import MediaWiki\n",
    "from mediawiki import PageError\n",
    "from mediawiki import MediaWikiCategoryTreeError\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import save_npz\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "stopwords_ru = set(stopwords.words('russian'))\n",
    "tknzr = TweetTokenizer()\n",
    "morph = pm.MorphAnalyzer()\n",
    "wiki = MediaWiki(url='http://ru.wikipedia.org/w/api.php')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AA', 'AB', 'AD', 'AE', 'AF', 'AI', 'AJ', 'AK', 'AQ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:23<00:00,  2.62s/it]\n"
     ]
    }
   ],
   "source": [
    "path = listdir('C:/Users/Maxim/MyPy/wta/c')\n",
    "path = sorted(path)[:9]\n",
    "#path = ['AA', 'AD', 'AE']\n",
    "print(path)\n",
    "\n",
    "data = []\n",
    "for p in tqdm(path):\n",
    "    with jsonlines.open('C:/Users/Maxim/MyPy/wta/c/' + p, 'r') as f:\n",
    "        for entry in f:\n",
    "            data.append(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разбираемся с категориями. Добавялем категории, который есть в ok, но которых нет в дереве/dag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_graph = nx.read_gpickle('sources/graph_gpickle.gpickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 719813/719813 [00:06<00:00, 103830.36it/s]\n"
     ]
    }
   ],
   "source": [
    "links = [link for link in jsonlines.open('links/link_directed.txt', 'r')]\n",
    "categories_id_dict = {}\n",
    "id_categories_dict = {}\n",
    "for link in tqdm(links):\n",
    "    id_categories_dict[link['node_1_id']] = re.sub(r'^Категория:', '', link['node_1_title'])\n",
    "    id_categories_dict[link['node_2_id']] = re.sub(r'^Категория:', '', link['node_2_title'])\n",
    "    \n",
    "    categories_id_dict[re.sub(r'^Категория:', '', link['node_1_title'])] = link['node_1_id']\n",
    "    categories_id_dict[re.sub(r'^Категория:', '', link['node_2_title'])] = link['node_2_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 116584/116584 [00:01<00:00, 73219.49it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in trange(len(data)):\n",
    "    categories = data[i]['categories']\n",
    "    categories_nu = set()\n",
    "    for c in categories:\n",
    "        c = re.sub(r'\\u200e', '', c)\n",
    "        c = re.sub(r'\\xa0', ' ', c)\n",
    "        categories_nu.add(re.sub(r'\\|.*', '', c))\n",
    "    data[i]['categories'] = categories_nu\n",
    "    \n",
    "categories = [d['categories'] for d in data]    \n",
    "categories = set(list(itertools.chain(*categories)))\n",
    "\n",
    "categories_dict = {c: 0 for c in categories}\n",
    "for d in data:\n",
    "    for c in d['categories']:\n",
    "        if categories_dict.get(c) is not None:\n",
    "            categories_dict[c] += 1\n",
    "            \n",
    "ok_set = {c for c in categories if categories_dict[c] > 5 and categories_dict[c] < 5000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 14857/14857 [00:00<00:00, 1058227.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "557"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missed = []\n",
    "cat_graph_nodes = {id_categories_dict[cat] for cat in cat_graph.nodes()}\n",
    "for c in tqdm(ok_set):\n",
    "    if c not in cat_graph_nodes:\n",
    "        missed.append(c)\n",
    "\n",
    "len(missed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 557/557 [15:44<00:00,  1.70s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doomed = set()\n",
    "restored = set()\n",
    "\n",
    "for miss in tqdm(missed):\n",
    "    flag = True\n",
    "    while flag:\n",
    "        try:\n",
    "            res = wiki.categorytree(category=miss, depth=1)\n",
    "            flag = False\n",
    "        except PageError:\n",
    "            try:\n",
    "                res = wiki.categorytree(category='Категория:'+miss, depth=1)\n",
    "                flag = False\n",
    "            except PageError:\n",
    "                doomed.add(miss)\n",
    "                flag = False\n",
    "        except MediaWikiCategoryTreeError:\n",
    "            sleep(3)\n",
    "            \n",
    "    parents = res[list(res.keys())[0]]['parent-categories']\n",
    "    parents = [re.sub(r'^Категория:', '', p) for p in parents]\n",
    "    for parent in parents:\n",
    "        if parent in cat_graph_nodes:\n",
    "            flag = True\n",
    "            new_node_id = max(cat_graph.nodes()) + 1\n",
    "            cat_graph.add_edge(categories_id_dict[parent], new_node_id)\n",
    "            id_categories_dict[new_node_id] = miss\n",
    "            categories_id_dict[miss] = new_node_id\n",
    "            restored.add((parent, miss))\n",
    "            \n",
    "    if not flag:\n",
    "        doomed.add(miss)\n",
    "\n",
    "\n",
    "len(doomed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sources/id_cat.json\", mode=\"w\") as output:\n",
    "    output.write(json.dumps(id_categories_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sources/cat_id.json\", mode=\"w\") as output:\n",
    "    output.write(json.dumps(categories_id_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 116584/116584 [00:01<00:00, 87796.51it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 14697/14697 [03:24<00:00, 72.02it/s]\n"
     ]
    }
   ],
   "source": [
    "ok_set = {c for c in ok_set if c not in doomed}\n",
    "ok = sorted(ok_set)\n",
    "with open(\"sources/accepted_categories.txt\", mode=\"w\", encoding=\"utf-8\") as output:\n",
    "    for c in ok:\n",
    "        output.write(c + '\\n')\n",
    "\n",
    "article_category_dict = {}\n",
    "for d in tqdm(data):\n",
    "    cats = [c for c in d['categories'] if c in ok_set]\n",
    "    if len(cats) > 0:\n",
    "        article_category_dict[d['id']] = cats\n",
    "with open(\"sources/article_cat.json\", mode=\"w\") as output:\n",
    "    output.write(json.dumps(article_category_dict))\n",
    "\n",
    "category_article_dict = {c: [d['id'] for d in data if c in d['categories']] for c in tqdm(ok_set)}\n",
    "with open(\"sources/cat_article.json\", mode=\"w\") as output:\n",
    "    output.write(json.dumps(category_article_dict))\n",
    "    \n",
    "nx.write_gpickle(cat_graph, 'sources/graph_gpickle.gpickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Формируем тексты (лемматизированные и просто \"очищенные\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 116584/116584 [05:07<00:00, 378.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "ids = article_category_dict.keys()\n",
    "\n",
    "with jsonlines.open('sources/texts.jl', 'w') as output:\n",
    "    for d in tqdm(data):\n",
    "        if d['id'] in ids:\n",
    "            text = re.sub(r'\\n+', '. ', d['text'])\n",
    "            text = re.sub(r'\\xa0', ' ', text)\n",
    "            tokens = tknzr.tokenize(text=text)\n",
    "            tokens = [t.lower() for t in tokens if t.lower() not in stopwords_ru]     #  if t.isalnum()\n",
    "            text = ' '.join(tokens)\n",
    "            output.write({'text': text, 'id': d['id']})\n",
    "            words.extend(sorted(set([t for t in tokens if t.isalpha()])))\n",
    "\n",
    "print(\"Done\")\n",
    "words = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 1231449/1231449 [06:24<00:00, 3200.89it/s]\n"
     ]
    }
   ],
   "source": [
    "normal_forms = {}\n",
    "for w in tqdm(words):\n",
    "    nf = morph.parse(w)[0].normal_form\n",
    "    normal_forms[w] = nf\n",
    "    \n",
    "with open(\"sources/normal_forms.json\", mode=\"w\") as output:\n",
    "    output.write(json.dumps(normal_forms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96794it [01:02, 1545.66it/s]\n"
     ]
    }
   ],
   "source": [
    "with jsonlines.open('sources/texts.jl', 'r') as file:\n",
    "    with jsonlines.open('sources/normalized_texts.jl', 'w') as output:\n",
    "        for f in tqdm(file):\n",
    "            id = f['id']\n",
    "            text = f['text']\n",
    "            tokens = re.split(r' ', text)\n",
    "            tokens = [normal_forms.get(t) for t in tokens if normal_forms.get(t) is not None]\n",
    "            text = ' '.join(tokens)\n",
    "            output.write({'text': text, 'id': id})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготавливаем заголовки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = set(texts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 116584/116584 [01:05<00:00, 1772.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "id_title = dict()\n",
    "unsup_symb = set('.,\\(\\):«»?!')\n",
    "sw_ru = nltk.corpus.stopwords.words('russian')\n",
    "\n",
    "for i in trange(len(data)):\n",
    "    id = data[i]['id']\n",
    "    if id in ids:\n",
    "        tokens = tknzr.tokenize(text=data[i]['title'])\n",
    "        tokens = [t.lower() for t in tokens if t not in unsup_symb]\n",
    "        tokens = [morph.parse(t)[0].normal_form for t in tokens]\n",
    "        tokens = [t for t in tokens if t not in sw_ru]\n",
    "        if len(tokens) > 0:\n",
    "            id_title[id] = ' '.join(tokens)\n",
    "        else:\n",
    "            id_title[id] = ' '\n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [y for (t, y) in sorted(id_title.items())]\n",
    "X_titles = TfidfVectorizer(min_df=5, max_df=0.98).fit_transform(titles)\n",
    "X_titles_bigrams = TfidfVectorizer(ngram_range = (2,2)).fit_transform(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_npz(\"sources/X_titles\", X_titles)\n",
    "save_npz(\"sources/X_titles_bigrams\", X_titles_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Формируем tf-idf и svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = {item['id']: item['text'] for item in jsonlines.open('sources/normalized_texts.jl', 'r')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=5, max_df=0.98)\n",
    "X_tfidf = vectorizer.fit_transform([text for (id, text) in sorted(texts.items())])\n",
    "save_npz(\"sources/tf_idf\", X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=500, random_state=27)\n",
    "X_svd = svd.fit_transform(X_tfidf)\n",
    "np.save(\"sources/svd500\", X_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=1000, random_state=27)\n",
    "X_svd = svd.fit_transform(X_tfidf)\n",
    "np.save(\"sources/svd1000\", X_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=5000, random_state=27)\n",
    "X_svd = svd.fit_transform(X_tfidf)\n",
    "np.save(\"sources/svd5000\", X_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96794, 124767)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = load_npz(\"sources/tf_idf.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<96794x124767 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 18742443 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
